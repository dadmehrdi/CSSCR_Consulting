{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ee73c7",
   "metadata": {},
   "source": [
    "# Toxic Femininity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda0a6a",
   "metadata": {},
   "source": [
    "Web Scraping Script for incels.is Forum Posts\n",
    "This script extracts the titles, replies, and views from the incels.is forum posts under a specific category. It navigates through all pages of the category until there are no more pages left. The extracted data is then saved to an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449d3e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved titles, replies, and views to titles_replies_views.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://incels.is\"\n",
    "forum_url = \"/forums/inceldom-discussion.2/?prefix_id=23\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "title_slugs = []\n",
    "reply_counts = []\n",
    "view_counts = []\n",
    "\n",
    "while forum_url:\n",
    "    response = requests.get(base_url + forum_url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {base_url + forum_url}\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title_elements = soup.find_all('a', {'data-xf-init': 'preview-tooltip'})\n",
    "    data_elements = soup.find_all('dd')\n",
    "\n",
    "    for title in title_elements:\n",
    "        preview_url = title['data-preview-url']\n",
    "        title_slug = preview_url.split('/')[-2]\n",
    "        title_slugs.append(title_slug)\n",
    "\n",
    "    for i in range(0, len(data_elements), 2):\n",
    "        reply_counts.append(data_elements[i].text.strip())\n",
    "        view_counts.append(data_elements[i+1].text.strip())\n",
    "\n",
    "    # Check for the \"Next\" button and retrieve the next page's URL\n",
    "    next_button = soup.find('a', {'class': 'pageNav-jump pageNav-jump--next'})\n",
    "    forum_url = next_button['href'] if next_button else None\n",
    "\n",
    "# Convert lists to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': title_slugs,\n",
    "    'Replies': reply_counts,\n",
    "    'Views': view_counts\n",
    "})\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df.to_excel(\"titles_replies_views.xlsx\", index=False)\n",
    "\n",
    "print(\"Saved titles, replies, and views to titles_replies_views.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db097c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
